# Network
MODEL_USE: mlvqa

QUES_LAYER: 3
IMG_LAYER: 1
LAYER: 3
HIDDEN_SIZE: 512
FF_SIZE: 2048
MULTI_HEAD: 8
DROPOUT_R: 0.1
FLAT_MLP_SIZE: 512
FLAT_GLIMPSES: 1
FLAT_OUT_SIZE: 1024
MAX_TOKEN: 29

# Execution
BATCH_SIZE: 64
LR_BASE: 0.00012
LR_DECAY_R: 0.2
LR_DECAY_LIST: [10, 12]
WARMUP_EPOCH: 3
MAX_EPOCH: 14
GRAD_NORM_CLIP: -1
GRAD_ACCU_STEPS: 1
LOSS_FUNC: ce
LOSS_FUNC_MSK: ce
LOSS_REDUCTION: sum
OPT: Adam
OPT_PARAMS: {betas: '(0.9, 0.98)', eps: '1e-9'}
BETA: 0.2

MASK_SUM: 6
VERB_PROB: 0.42
NOUN_PROB: 0.49
PRON_PROB_IMG: 0.08
NUM_PROB: 0.01

PRON_PROB_QUES: 0.11
DET_PROB: 0.44
ADP_PROB: 0.20
CONJ_PROB: 0.01
ADV_PROB: 0.10
PRT_PROB: 0.02
ADJ_PROB: 0.12
