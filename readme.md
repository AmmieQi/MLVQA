[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)
The codes of our method is based on the [openvqa](https://github.com/MILVLG/openvqa/blob/master/README.md), i.e., only the mlvqa method is developed by jie ma. see the details in the [openvqa](https://github.com/MILVLG/openvqa/blob/master/README.md). We propose a multitask learning framework for visual question answering via intra- and inter-modality modeling named MLVQA.

installation and requirements
----
all the required packages are the same as openvqa. 

License
----
This project is released under the [Apache 2.0 license](https://github.com/MILVLG/openvqa/blob/master/LICENSE).